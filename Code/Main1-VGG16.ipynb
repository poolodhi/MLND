{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Imports to view data\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# Visualization\n",
    "from keras.utils import print_summary\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#ML libraries\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory paths\n",
    "TRAIN_DIR = \"../Dataset/asl_alphabet_train/asl_alphabet_train\"\n",
    "TEST_DIR = \"../Dataset/asl_alphabet_test\"\n",
    "MODEL_DIR = './Model'\n",
    "MODEL_PATH = MODEL_DIR+\"/Model1.h5\"\n",
    "MODEL_WEIGHT_PATH = MODEL_DIR+\"/Model_Weight1.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global variables\n",
    "TARGET_SIZE = (64, 64)\n",
    "TARGET_DIMS = (64, 64, 3) # add channel for RGB\n",
    "CLASSES = 29\n",
    "VALIDATION_SPLIT = 0.1\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 78300 images belonging to 29 classes.\n",
      "Found 8700 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "#Load Train dataset\n",
    "train_image_generator = ImageDataGenerator(\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "validation_image_generator = ImageDataGenerator(\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True,\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "train_generator = train_image_generator.flow_from_directory(TRAIN_DIR, target_size=TARGET_SIZE, batch_size=BATCH_SIZE, shuffle=True, subset=\"training\")\n",
    "val_generator = validation_image_generator.flow_from_directory(TRAIN_DIR, target_size=TARGET_SIZE, batch_size=BATCH_SIZE, subset=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Model VGG16 Model\n",
    "def VGG16_model_build():\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    from keras.layers import Input\n",
    "\n",
    "    input_tensor = Input(shape=TARGET_DIMS)\n",
    "    model = VGG16(input_tensor = input_tensor, weights=None, include_top=True, classes= CLASSES)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SHIV\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "base_model = VGG16_model_build()\n",
    "x = base_model.output\n",
    "#x = (Dense(CLASSES, activation='softmax'))(x)\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 29)                118813    \n",
      "=================================================================\n",
      "Total params: 40,007,517\n",
      "Trainable params: 40,007,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SHIV\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "39/39 [==============================] - 313s 8s/step - loss: 15.2746 - acc: 0.0329 - val_loss: 15.5137 - val_acc: 0.0375\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 15.51367, saving model to ./Model/Model_Weight1.h5\n",
      "Epoch 2/20\n",
      "39/39 [==============================] - 320s 8s/step - loss: 15.6273 - acc: 0.0304 - val_loss: 15.6648 - val_acc: 0.0281\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 15.51367\n",
      "Epoch 3/20\n",
      "39/39 [==============================] - 325s 8s/step - loss: 15.5627 - acc: 0.0345 - val_loss: 15.5640 - val_acc: 0.0344\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 15.51367\n",
      "Epoch 4/20\n",
      "39/39 [==============================] - 318s 8s/step - loss: 15.6079 - acc: 0.0317 - val_loss: 15.4633 - val_acc: 0.0406\n",
      "\n",
      "Epoch 00004: val_loss improved from 15.51367 to 15.46330, saving model to ./Model/Model_Weight1.h5\n",
      "Epoch 5/20\n",
      "39/39 [==============================] - 322s 8s/step - loss: 15.5369 - acc: 0.0361 - val_loss: 15.4633 - val_acc: 0.0406\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 15.46330\n",
      "Epoch 6/20\n",
      "39/39 [==============================] - 325s 8s/step - loss: 15.5240 - acc: 0.0369 - val_loss: 15.7151 - val_acc: 0.0250\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 15.46330\n",
      "Epoch 7/20\n",
      "39/39 [==============================] - 330s 8s/step - loss: 15.5563 - acc: 0.0349 - val_loss: 15.1611 - val_acc: 0.0594\n",
      "\n",
      "Epoch 00007: val_loss improved from 15.46330 to 15.16108, saving model to ./Model/Model_Weight1.h5\n",
      "Epoch 8/20\n",
      "39/39 [==============================] - 314s 8s/step - loss: 15.5240 - acc: 0.0369 - val_loss: 15.5640 - val_acc: 0.0344\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 15.16108\n",
      "Epoch 9/20\n",
      "39/39 [==============================] - 330s 8s/step - loss: 15.5175 - acc: 0.0373 - val_loss: 15.8159 - val_acc: 0.0187\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 15.16108\n",
      "Epoch 10/20\n",
      "39/39 [==============================] - 347s 9s/step - loss: 15.5498 - acc: 0.0353 - val_loss: 15.4633 - val_acc: 0.0406\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 15.16108\n",
      "Epoch 11/20\n",
      "39/39 [==============================] - 336s 9s/step - loss: 15.4982 - acc: 0.0385 - val_loss: 15.3122 - val_acc: 0.0500\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 15.16108\n",
      "Epoch 12/20\n",
      "39/39 [==============================] - 325s 8s/step - loss: 15.5305 - acc: 0.0365 - val_loss: 15.5640 - val_acc: 0.0344\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 15.16108\n",
      "Epoch 13/20\n",
      "39/39 [==============================] - 323s 8s/step - loss: 15.6209 - acc: 0.0308 - val_loss: 15.7151 - val_acc: 0.0250\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 15.16108\n",
      "Epoch 14/20\n",
      "39/39 [==============================] - 339s 9s/step - loss: 15.4853 - acc: 0.0393 - val_loss: 15.6144 - val_acc: 0.0312\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 15.16108\n",
      "Epoch 15/20\n",
      "39/39 [==============================] - 338s 9s/step - loss: 15.4788 - acc: 0.0397 - val_loss: 15.4633 - val_acc: 0.0406\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 15.16108\n",
      "Epoch 16/20\n",
      "39/39 [==============================] - 333s 9s/step - loss: 15.5950 - acc: 0.0325 - val_loss: 15.7151 - val_acc: 0.0250\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 15.16108\n",
      "Epoch 17/20\n",
      "39/39 [==============================] - 320s 8s/step - loss: 15.6532 - acc: 0.0288 - val_loss: 15.5137 - val_acc: 0.0375\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 15.16108\n",
      "Epoch 18/20\n",
      "39/39 [==============================] - 322s 8s/step - loss: 15.5434 - acc: 0.0357 - val_loss: 15.6144 - val_acc: 0.0312\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 15.16108\n",
      "Epoch 19/20\n",
      "39/39 [==============================] - 320s 8s/step - loss: 15.5434 - acc: 0.0357 - val_loss: 15.5137 - val_acc: 0.0375\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 15.16108\n",
      "Epoch 20/20\n",
      "39/39 [==============================] - 323s 8s/step - loss: 15.5111 - acc: 0.0377 - val_loss: 15.6648 - val_acc: 0.0281\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 15.16108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25345c689e8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checkpointer to save the best models\n",
    "checkpointer = ModelCheckpoint(filepath=MODEL_WEIGHT_PATH, \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "steps_per_epoch = int( np.ceil(len(train_generator)*2 / BATCH_SIZE) )\n",
    "validation_steps = int( np.ceil(len(val_generator)*2 / BATCH_SIZE) )\n",
    "\n",
    "model.fit_generator(train_generator, validation_data=val_generator, \n",
    "                    steps_per_epoch =  steps_per_epoch,\n",
    "                    validation_steps = validation_steps,\n",
    "                    epochs=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J'] 64\n",
      "15.614404678344727 , 0.03125\n"
     ]
    }
   ],
   "source": [
    "#Predict on validation dataset\n",
    "predictions = model.predict_generator(val_generator, steps=1)        \n",
    "predictions = np.argmax(predictions, axis=-1) #multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
    "\n",
    "predictions = [label_map[k] for k in predictions]\n",
    "\n",
    "print(predictions, len(predictions))\n",
    "\n",
    "loss, acc = model.evaluate_generator(val_generator, steps=1, verbose=0)\n",
    "\n",
    "print(loss,\",\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 images belonging to 1 classes.\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "test_image_generator = ImageDataGenerator(\n",
    "    samplewise_center = True,\n",
    "    samplewise_std_normalization = True,\n",
    ")\n",
    "\n",
    "test_generator = test_image_generator.flow_from_directory(TEST_DIR, target_size=TARGET_SIZE, batch_size=28, shuffle=False, \n",
    "    class_mode='categorical')\n",
    "print(test_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J'] 28\n"
     ]
    }
   ],
   "source": [
    "#Predict\n",
    "test_generator.reset()\n",
    "predictions = model.predict_generator(test_generator, steps=1)\n",
    "predictions = np.argmax(predictions, axis=1) #multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
    "\n",
    "predictions = [label_map[k] for k in predictions]\n",
    "\n",
    "print(predictions, len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Filename Predictions\n",
      "0         asl_alphabet_test\\A_test.jpg           J\n",
      "1         asl_alphabet_test\\B_test.jpg           J\n",
      "2         asl_alphabet_test\\C_test.jpg           J\n",
      "3         asl_alphabet_test\\D_test.jpg           J\n",
      "4         asl_alphabet_test\\E_test.jpg           J\n",
      "5         asl_alphabet_test\\F_test.jpg           J\n",
      "6         asl_alphabet_test\\G_test.jpg           J\n",
      "7         asl_alphabet_test\\H_test.jpg           J\n",
      "8         asl_alphabet_test\\I_test.jpg           J\n",
      "9         asl_alphabet_test\\J_test.jpg           J\n",
      "10        asl_alphabet_test\\K_test.jpg           J\n",
      "11        asl_alphabet_test\\L_test.jpg           J\n",
      "12        asl_alphabet_test\\M_test.jpg           J\n",
      "13        asl_alphabet_test\\N_test.jpg           J\n",
      "14        asl_alphabet_test\\O_test.jpg           J\n",
      "15        asl_alphabet_test\\P_test.jpg           J\n",
      "16        asl_alphabet_test\\Q_test.jpg           J\n",
      "17        asl_alphabet_test\\R_test.jpg           J\n",
      "18        asl_alphabet_test\\S_test.jpg           J\n",
      "19        asl_alphabet_test\\T_test.jpg           J\n",
      "20        asl_alphabet_test\\U_test.jpg           J\n",
      "21        asl_alphabet_test\\V_test.jpg           J\n",
      "22        asl_alphabet_test\\W_test.jpg           J\n",
      "23        asl_alphabet_test\\X_test.jpg           J\n",
      "24        asl_alphabet_test\\Y_test.jpg           J\n",
      "25        asl_alphabet_test\\Z_test.jpg           J\n",
      "26  asl_alphabet_test\\nothing_test.jpg           J\n",
      "27    asl_alphabet_test\\space_test.jpg           J\n"
     ]
    }
   ],
   "source": [
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 3.571428571428571\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for file,prediction in zip(filenames,predictions):\n",
    "    #print(file,prediction)\n",
    "    if(prediction+'_test' in file):\n",
    "        count+=1\n",
    "        \n",
    "print(\"accuracy\",count/len(filenames)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 29)                118813    \n",
      "=================================================================\n",
      "Total params: 40,007,517\n",
      "Trainable params: 40,007,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define Model VGG16 Model with pretrained weight\n",
    "def VGG16_model_build_weight():\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    from keras.layers import Input\n",
    "\n",
    "    input_tensor = Input(shape=TARGET_DIMS)\n",
    "    model = VGG16(input_tensor = input_tensor, weights='imagenet', include_top=False)\n",
    "    return model\n",
    "\n",
    "from keras.models import Model\n",
    "weight_base_model = VGG16_model_build_weight()\n",
    "x1 = weight_base_model.output\n",
    "#Add the fully-connected layers \n",
    "x1 = Flatten(name='flatten')(x1)\n",
    "x1 = Dense(4096, activation='relu', name='fc1')(x1)\n",
    "x1 = Dense(4096, activation='relu', name='fc2')(x1)\n",
    "x1 = Dense(CLASSES, activation='softmax', name='predictions')(x1)\n",
    "weight_model = Model(inputs=weight_base_model.input, outputs=x1)\n",
    "\n",
    "#Compile the model\n",
    "weight_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "weight_model.summary()\n",
    "MODEL_PATH = MODEL_DIR+\"/Model1-withweight.h5\"\n",
    "weight_model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "39/39 [==============================] - 327s 8s/step - loss: 15.2754 - acc: 0.0337 - val_loss: 15.3122 - val_acc: 0.0500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 15.31219, saving model to ./Model/Model_Weight1-withweight.h5\n",
      "Epoch 2/10\n",
      "39/39 [==============================] - 321s 8s/step - loss: 15.6144 - acc: 0.0312 - val_loss: 15.3122 - val_acc: 0.0500\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 15.31219\n",
      "Epoch 3/10\n",
      "39/39 [==============================] - 332s 9s/step - loss: 15.5757 - acc: 0.0337 - val_loss: 15.3626 - val_acc: 0.0469\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 15.31219\n",
      "Epoch 4/10\n",
      "39/39 [==============================] - 317s 8s/step - loss: 15.5434 - acc: 0.0357 - val_loss: 15.5060 - val_acc: 0.0380\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 15.31219\n",
      "Epoch 5/10\n",
      "39/39 [==============================] - 317s 8s/step - loss: 15.5821 - acc: 0.0333 - val_loss: 15.4129 - val_acc: 0.0437\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 15.31219\n",
      "Epoch 6/10\n",
      "39/39 [==============================] - 320s 8s/step - loss: 15.5692 - acc: 0.0341 - val_loss: 15.3626 - val_acc: 0.0469\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 15.31219\n",
      "Epoch 7/10\n",
      "39/39 [==============================] - 322s 8s/step - loss: 15.5369 - acc: 0.0361 - val_loss: 15.8663 - val_acc: 0.0156\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 15.31219\n",
      "Epoch 8/10\n",
      "39/39 [==============================] - 323s 8s/step - loss: 15.5240 - acc: 0.0369 - val_loss: 15.7151 - val_acc: 0.0250\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 15.31219\n",
      "Epoch 9/10\n",
      "39/39 [==============================] - 322s 8s/step - loss: 15.6467 - acc: 0.0292 - val_loss: 15.6144 - val_acc: 0.0312\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 15.31219\n",
      "Epoch 10/10\n",
      "39/39 [==============================] - 321s 8s/step - loss: 15.6144 - acc: 0.0312 - val_loss: 15.5640 - val_acc: 0.0344\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 15.31219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x253452c20b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checkpointer to save the best models\n",
    "MODEL_WEIGHT_PATH = MODEL_DIR+\"/Model_Weight1-withweight.h5\"\n",
    "checkpointer = ModelCheckpoint(filepath=MODEL_WEIGHT_PATH, \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "steps_per_epoch = int( np.ceil(len(train_generator)*2 / BATCH_SIZE) )\n",
    "validation_steps = int( np.ceil(len(val_generator)*2 / BATCH_SIZE) )\n",
    "\n",
    "weight_model.fit_generator(train_generator, validation_data=val_generator, \n",
    "                    steps_per_epoch =  steps_per_epoch,\n",
    "                    validation_steps = validation_steps,\n",
    "                    epochs=10, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D'] 64\n",
      "15.866250038146973 , 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Predict on validation dataset\n",
    "predictions = weight_model.predict_generator(val_generator, steps=1)        \n",
    "predictions = np.argmax(predictions, axis=-1) #multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
    "\n",
    "predictions = [label_map[k] for k in predictions]\n",
    "\n",
    "print(predictions, len(predictions))\n",
    "\n",
    "loss, acc = weight_model.evaluate_generator(val_generator, steps=1, verbose=0)\n",
    "\n",
    "print(loss,\",\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 images belonging to 1 classes.\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "test_image_generator = ImageDataGenerator(\n",
    "    samplewise_center = True,\n",
    "    samplewise_std_normalization = True,\n",
    ")\n",
    "\n",
    "test_generator = test_image_generator.flow_from_directory(TEST_DIR, target_size=TARGET_SIZE, batch_size=28, shuffle=False, \n",
    "    class_mode='categorical')\n",
    "print(test_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D'] 28\n"
     ]
    }
   ],
   "source": [
    "#Predict\n",
    "test_generator.reset()\n",
    "predictions = weight_model.predict_generator(test_generator, steps=1)\n",
    "predictions = np.argmax(predictions, axis=1) #multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
    "\n",
    "predictions = [label_map[k] for k in predictions]\n",
    "\n",
    "print(predictions, len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 3.571428571428571\n"
     ]
    }
   ],
   "source": [
    "filenames=test_generator.filenames\n",
    "count = 0\n",
    "for file,prediction in zip(filenames,predictions):\n",
    "    #print(file,prediction)\n",
    "    if(prediction+'_test' in file):\n",
    "        count+=1\n",
    "        \n",
    "print(\"accuracy\",count/len(filenames)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

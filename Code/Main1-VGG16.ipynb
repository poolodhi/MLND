{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Imports to view data\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# Visualization\n",
    "from keras.utils import print_summary\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#ML libraries\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory paths\n",
    "TRAIN_DIR = \"../Dataset/asl_alphabet_train/asl_alphabet_train\"\n",
    "TEST_DIR = \"../Dataset/asl_alphabet_test\"\n",
    "MODEL_DIR = './Model'\n",
    "MODEL_PATH = MODEL_DIR+\"/Model1.h5\"\n",
    "MODEL_WEIGHT_PATH = MODEL_DIR+\"/Model_Weight1.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global variables\n",
    "TARGET_SIZE = (64, 64)\n",
    "TARGET_DIMS = (64, 64, 3) # add channel for RGB\n",
    "CLASSES = 29\n",
    "VALIDATION_SPLIT = 0.1\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 78300 images belonging to 29 classes.\n",
      "Found 8700 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "#Load Train dataset\n",
    "train_image_generator = ImageDataGenerator(\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "validation_image_generator = ImageDataGenerator(\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True,\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "train_generator = train_image_generator.flow_from_directory(TRAIN_DIR, target_size=TARGET_SIZE, batch_size=BATCH_SIZE, shuffle=True, subset=\"training\")\n",
    "val_generator = validation_image_generator.flow_from_directory(TRAIN_DIR, target_size=TARGET_SIZE, batch_size=BATCH_SIZE, subset=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Model VGG16 Model\n",
    "def VGG16_model_build():\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    from keras.layers import Input\n",
    "\n",
    "    input_tensor = Input(shape=TARGET_DIMS)\n",
    "    model = VGG16(input_tensor = input_tensor, weights=None, include_top=True, classes= CLASSES)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "base_model = VGG16_model_build()\n",
    "x = base_model.output\n",
    "#x = (Dense(CLASSES, activation='softmax'))(x)\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 29)                118813    \n",
      "=================================================================\n",
      "Total params: 40,007,517\n",
      "Trainable params: 40,007,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "77/77 [==============================] - 574s 7s/step - loss: 15.6013 - acc: 0.0321 - val_loss: 15.5305 - val_acc: 0.0365\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 15.53046, saving model to ./Model/Model_Weight1.h5\n",
      "Epoch 2/20\n",
      "77/77 [==============================] - 585s 8s/step - loss: 15.5326 - acc: 0.0363 - val_loss: 15.6704 - val_acc: 0.0278\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 15.53046\n",
      "Epoch 3/20\n",
      "77/77 [==============================] - 591s 8s/step - loss: 15.5326 - acc: 0.0363 - val_loss: 15.6984 - val_acc: 0.0260\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 15.53046\n",
      "Epoch 4/20\n",
      "77/77 [==============================] - 598s 8s/step - loss: 15.5653 - acc: 0.0343 - val_loss: 15.4465 - val_acc: 0.0417\n",
      "\n",
      "Epoch 00004: val_loss improved from 15.53046 to 15.44651, saving model to ./Model/Model_Weight1.h5\n",
      "Epoch 5/20\n",
      "77/77 [==============================] - 603s 8s/step - loss: 15.5359 - acc: 0.0361 - val_loss: 15.3346 - val_acc: 0.0486\n",
      "\n",
      "Epoch 00005: val_loss improved from 15.44651 to 15.33458, saving model to ./Model/Model_Weight1.h5\n",
      "Epoch 6/20\n",
      "77/77 [==============================] - 604s 8s/step - loss: 15.6046 - acc: 0.0319 - val_loss: 15.4465 - val_acc: 0.0417\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 15.33458\n",
      "Epoch 7/20\n",
      "77/77 [==============================] - 600s 8s/step - loss: 15.5555 - acc: 0.0349 - val_loss: 15.4700 - val_acc: 0.0402\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 15.33458\n",
      "Epoch 8/20\n",
      "77/77 [==============================] - 602s 8s/step - loss: 15.5817 - acc: 0.0333 - val_loss: 15.2226 - val_acc: 0.0556\n",
      "\n",
      "Epoch 00008: val_loss improved from 15.33458 to 15.22265, saving model to ./Model/Model_Weight1.h5\n",
      "Epoch 9/20\n",
      "77/77 [==============================] - 607s 8s/step - loss: 15.5261 - acc: 0.0367 - val_loss: 15.3626 - val_acc: 0.0469\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 15.22265\n",
      "Epoch 10/20\n",
      "77/77 [==============================] - 607s 8s/step - loss: 15.5817 - acc: 0.0333 - val_loss: 15.4185 - val_acc: 0.0434\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 15.22265\n",
      "Epoch 11/20\n",
      "77/77 [==============================] - 601s 8s/step - loss: 15.6004 - acc: 0.0321 - val_loss: 15.3905 - val_acc: 0.0451\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 15.22265\n",
      "Epoch 12/20\n",
      "77/77 [==============================] - 588s 8s/step - loss: 15.5752 - acc: 0.0337 - val_loss: 15.6424 - val_acc: 0.0295\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 15.22265\n",
      "Epoch 13/20\n",
      "77/77 [==============================] - 587s 8s/step - loss: 15.5359 - acc: 0.0361 - val_loss: 15.6704 - val_acc: 0.0278\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 15.22265\n",
      "Epoch 14/20\n",
      "77/77 [==============================] - 586s 8s/step - loss: 15.6275 - acc: 0.0304 - val_loss: 15.8942 - val_acc: 0.0139\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 15.22265\n",
      "Epoch 15/20\n",
      "77/77 [==============================] - 587s 8s/step - loss: 15.5359 - acc: 0.0361 - val_loss: 15.7263 - val_acc: 0.0243\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 15.22265\n",
      "Epoch 16/20\n",
      "77/77 [==============================] - 589s 8s/step - loss: 15.5784 - acc: 0.0335 - val_loss: 15.4745 - val_acc: 0.0399\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 15.22265\n",
      "Epoch 17/20\n",
      "77/77 [==============================] - 595s 8s/step - loss: 15.5032 - acc: 0.0381 - val_loss: 15.6984 - val_acc: 0.0260\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 15.22265\n",
      "Epoch 18/20\n",
      "77/77 [==============================] - 609s 8s/step - loss: 15.5392 - acc: 0.0359 - val_loss: 15.6424 - val_acc: 0.0295\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 15.22265\n",
      "Epoch 19/20\n",
      "77/77 [==============================] - 635s 8s/step - loss: 15.5555 - acc: 0.0349 - val_loss: 15.2506 - val_acc: 0.0538\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 15.22265\n",
      "Epoch 20/20\n",
      "77/77 [==============================] - 598s 8s/step - loss: 15.5065 - acc: 0.0379 - val_loss: 15.5864 - val_acc: 0.0330\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 15.22265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cb087f2978>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checkpointer to save the best models\n",
    "checkpointer = ModelCheckpoint(filepath=MODEL_WEIGHT_PATH, \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "steps_per_epoch = int( np.ceil(len(train_generator)*2 / BATCH_SIZE) )\n",
    "validation_steps = int( np.ceil(len(val_generator)*2 / BATCH_SIZE) )\n",
    "\n",
    "model.fit_generator(train_generator, validation_data=val_generator, \n",
    "                    steps_per_epoch =  steps_per_epoch,\n",
    "                    validation_steps = validation_steps,\n",
    "                    epochs=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E'] 64\n",
      "15.866250038146973 , 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Predict on validation dataset\n",
    "predictions = model.predict_generator(val_generator, steps=1)        \n",
    "predictions = np.argmax(predictions, axis=-1) #multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
    "\n",
    "predictions = [label_map[k] for k in predictions]\n",
    "\n",
    "print(predictions, len(predictions))\n",
    "\n",
    "loss, acc = model.evaluate_generator(val_generator, steps=1, verbose=0)\n",
    "\n",
    "print(loss,\",\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 images belonging to 1 classes.\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "test_image_generator = ImageDataGenerator(\n",
    "    samplewise_center = True,\n",
    "    samplewise_std_normalization = True,\n",
    ")\n",
    "\n",
    "test_generator = test_image_generator.flow_from_directory(TEST_DIR, target_size=TARGET_SIZE, batch_size=28, shuffle=False, \n",
    "    class_mode='categorical')\n",
    "print(test_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E'] 28\n"
     ]
    }
   ],
   "source": [
    "#Predict\n",
    "test_generator.reset()\n",
    "predictions = model.predict_generator(test_generator, steps=1)\n",
    "predictions = np.argmax(predictions, axis=1) #multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
    "\n",
    "predictions = [label_map[k] for k in predictions]\n",
    "\n",
    "print(predictions, len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Filename Predictions\n",
      "0         asl_alphabet_test\\A_test.jpg           E\n",
      "1         asl_alphabet_test\\B_test.jpg           E\n",
      "2         asl_alphabet_test\\C_test.jpg           E\n",
      "3         asl_alphabet_test\\D_test.jpg           E\n",
      "4         asl_alphabet_test\\E_test.jpg           E\n",
      "5         asl_alphabet_test\\F_test.jpg           E\n",
      "6         asl_alphabet_test\\G_test.jpg           E\n",
      "7         asl_alphabet_test\\H_test.jpg           E\n",
      "8         asl_alphabet_test\\I_test.jpg           E\n",
      "9         asl_alphabet_test\\J_test.jpg           E\n",
      "10        asl_alphabet_test\\K_test.jpg           E\n",
      "11        asl_alphabet_test\\L_test.jpg           E\n",
      "12        asl_alphabet_test\\M_test.jpg           E\n",
      "13        asl_alphabet_test\\N_test.jpg           E\n",
      "14        asl_alphabet_test\\O_test.jpg           E\n",
      "15        asl_alphabet_test\\P_test.jpg           E\n",
      "16        asl_alphabet_test\\Q_test.jpg           E\n",
      "17        asl_alphabet_test\\R_test.jpg           E\n",
      "18        asl_alphabet_test\\S_test.jpg           E\n",
      "19        asl_alphabet_test\\T_test.jpg           E\n",
      "20        asl_alphabet_test\\U_test.jpg           E\n",
      "21        asl_alphabet_test\\V_test.jpg           E\n",
      "22        asl_alphabet_test\\W_test.jpg           E\n",
      "23        asl_alphabet_test\\X_test.jpg           E\n",
      "24        asl_alphabet_test\\Y_test.jpg           E\n",
      "25        asl_alphabet_test\\Z_test.jpg           E\n",
      "26  asl_alphabet_test\\nothing_test.jpg           E\n",
      "27    asl_alphabet_test\\space_test.jpg           E\n"
     ]
    }
   ],
   "source": [
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 3.571428571428571\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for file,prediction in zip(filenames,predictions):\n",
    "    #print(file,prediction)\n",
    "    if(prediction+'_test' in file):\n",
    "        count+=1\n",
    "        \n",
    "print(\"accuracy\",count/len(filenames)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@arindambaidya168/https-medium-com-arindambaidya168-using-keras-imagedatagenerator-b94a87cdefad\n",
    "#https://medium.com/@vijayabhaskar96/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 29)                118813    \n",
      "=================================================================\n",
      "Total params: 40,007,517\n",
      "Trainable params: 40,007,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define Model VGG16 Model with pretrained weight\n",
    "def VGG16_model_build_weight():\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    from keras.layers import Input\n",
    "\n",
    "    input_tensor = Input(shape=TARGET_DIMS)\n",
    "    model = VGG16(input_tensor = input_tensor, weights='imagenet', include_top=False)\n",
    "    return model\n",
    "\n",
    "from keras.models import Model\n",
    "weight_base_model = VGG16_model_build_weight()\n",
    "x1 = weight_base_model.output\n",
    "#Add the fully-connected layers \n",
    "x1 = Flatten(name='flatten')(x1)\n",
    "x1 = Dense(4096, activation='relu', name='fc1')(x1)\n",
    "x1 = Dense(4096, activation='relu', name='fc2')(x1)\n",
    "x1 = Dense(CLASSES, activation='softmax', name='predictions')(x1)\n",
    "weight_model = Model(inputs=weight_base_model.input, outputs=x1)\n",
    "\n",
    "#Compile the model\n",
    "weight_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "weight_model.summary()\n",
    "MODEL_PATH = MODEL_DIR+\"/Model1-withweight.h5\"\n",
    "weight_model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "39/39 [==============================] - 300s 8s/step - loss: 15.5434 - acc: 0.0357 - val_loss: 15.7151 - val_acc: 0.0250\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 15.71514, saving model to ./Model/Model_Weight1-withweight.h5\n",
      "Epoch 2/10\n",
      "39/39 [==============================] - 294s 8s/step - loss: 15.6661 - acc: 0.0280 - val_loss: 15.7655 - val_acc: 0.0219\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 15.71514\n",
      "Epoch 3/10\n",
      "39/39 [==============================] - 296s 8s/step - loss: 15.5950 - acc: 0.0325 - val_loss: 15.4633 - val_acc: 0.0406\n",
      "\n",
      "Epoch 00003: val_loss improved from 15.71514 to 15.46330, saving model to ./Model/Model_Weight1-withweight.h5\n",
      "Epoch 4/10\n",
      "39/39 [==============================] - 300s 8s/step - loss: 15.4788 - acc: 0.0397 - val_loss: 15.2115 - val_acc: 0.0563\n",
      "\n",
      "Epoch 00004: val_loss improved from 15.46330 to 15.21145, saving model to ./Model/Model_Weight1-withweight.h5\n",
      "Epoch 5/10\n",
      "39/39 [==============================] - 305s 8s/step - loss: 15.5886 - acc: 0.0329 - val_loss: 15.3626 - val_acc: 0.0469\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 15.21145\n",
      "Epoch 6/10\n",
      "39/39 [==============================] - 301s 8s/step - loss: 15.6079 - acc: 0.0317 - val_loss: 15.8159 - val_acc: 0.0187\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 15.21145\n",
      "Epoch 7/10\n",
      "39/39 [==============================] - 420s 11s/step - loss: 15.5498 - acc: 0.0353 - val_loss: 15.5137 - val_acc: 0.0375\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 15.21145\n",
      "Epoch 8/10\n",
      "39/39 [==============================] - 294s 8s/step - loss: 15.5950 - acc: 0.0325 - val_loss: 15.5640 - val_acc: 0.0344\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 15.21145\n",
      "Epoch 9/10\n",
      "39/39 [==============================] - 297s 8s/step - loss: 15.5240 - acc: 0.0369 - val_loss: 15.7151 - val_acc: 0.0250\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 15.21145\n",
      "Epoch 10/10\n",
      "39/39 [==============================] - 311s 8s/step - loss: 15.6015 - acc: 0.0321 - val_loss: 15.2115 - val_acc: 0.0563\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 15.21145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c1810a4c88>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checkpointer to save the best models\n",
    "MODEL_WEIGHT_PATH = MODEL_DIR+\"/Model_Weight1-withweight.h5\"\n",
    "checkpointer = ModelCheckpoint(filepath=MODEL_WEIGHT_PATH, \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "steps_per_epoch = int( np.ceil(len(train_generator)*2 / BATCH_SIZE) )\n",
    "validation_steps = int( np.ceil(len(val_generator)*2 / BATCH_SIZE) )\n",
    "\n",
    "weight_model.fit_generator(train_generator, validation_data=val_generator, \n",
    "                    steps_per_epoch =  steps_per_epoch,\n",
    "                    validation_steps = validation_steps,\n",
    "                    epochs=10, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S'] 64\n",
      "15.866250038146973 , 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Predict on validation dataset\n",
    "predictions = weight_model.predict_generator(val_generator, steps=1)        \n",
    "predictions = np.argmax(predictions, axis=-1) #multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
    "\n",
    "predictions = [label_map[k] for k in predictions]\n",
    "\n",
    "print(predictions, len(predictions))\n",
    "\n",
    "loss, acc = weight_model.evaluate_generator(val_generator, steps=1, verbose=0)\n",
    "\n",
    "print(loss,\",\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 images belonging to 1 classes.\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "test_image_generator = ImageDataGenerator(\n",
    "    samplewise_center = True,\n",
    "    samplewise_std_normalization = True,\n",
    ")\n",
    "\n",
    "test_generator = test_image_generator.flow_from_directory(TEST_DIR, target_size=TARGET_SIZE, batch_size=28, shuffle=False, \n",
    "    class_mode='categorical')\n",
    "print(test_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S'] 28\n"
     ]
    }
   ],
   "source": [
    "#Predict\n",
    "test_generator.reset()\n",
    "predictions = weight_model.predict_generator(test_generator, steps=1)\n",
    "predictions = np.argmax(predictions, axis=1) #multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
    "\n",
    "predictions = [label_map[k] for k in predictions]\n",
    "\n",
    "print(predictions, len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 3.571428571428571\n"
     ]
    }
   ],
   "source": [
    "filenames=test_generator.filenames\n",
    "count = 0\n",
    "for file,prediction in zip(filenames,predictions):\n",
    "    #print(file,prediction)\n",
    "    if(prediction+'_test' in file):\n",
    "        count+=1\n",
    "        \n",
    "print(\"accuracy\",count/len(filenames)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
